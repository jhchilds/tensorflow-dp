{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Privacy Final Project\n",
    "### Sam Clark & Josh Childs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project, we've decided to compare the accuracy of several normal Convolutional Neural Networks to their counter parts that will use differential privacy. We will be using the MNIST dataset with the tensflow library.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v2 as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from ipywidgets import IntProgress\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "tf.enable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.compat.v1.distributions import Laplace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load MNIST Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
    "input_shape = (28, 28, 1)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 26, 26, 28)        280       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 13, 13, 28)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 4732)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               605824    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 607,394\n",
      "Trainable params: 607,394\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "  Conv2D(28, kernel_size=(3,3), input_shape=input_shape),\n",
    "  MaxPooling2D(pool_size=(2, 2)),\n",
    "  Flatten(),\n",
    "  Dense(128, activation=tf.nn.relu),  \n",
    "  Dropout(0.3),\n",
    "  Dense(10,activation=tf.nn.softmax)\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train & Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
    "\n",
    "# model.compile(optimizer='sgd',\n",
    "#               loss='sparse_categorical_crossentropy', \n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# model.fit(x=x_train,y=y_train, epochs=1, callbacks = [callback])\n",
    "# model.save(\"models/original\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Differential Privacy Optimizer Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_l2_clip(v, b):\n",
    "    norm = tf.norm(v, ord=2)\n",
    "    return tf.cond(norm > b, lambda: b * (v / norm), lambda: v)\n",
    "\n",
    "def laplace_mech(v, sensitivity, epsilon):\n",
    "    return v + np.random.laplace(loc=0, scale=sensitivity / epsilon)\n",
    "\n",
    "def tf_laplace_mech(v, sensitivity, epsilon):\n",
    "    return tf.numpy_function(laplace_mech, [v, sensitivity, epsilon], tf.float32)\n",
    "\n",
    "def tf_gaussian_mech(v, sensitivity, epsilon, delta):\n",
    "    return v + tf.random.normal(v.shape, mean=0.0, stddev=sensitivity * np.sqrt(2*np.log(1.25/delta)) / epsilon)\n",
    "\n",
    "def tf_gaussian_mech_RDP(v, sensitivity, alpha, epsilon):\n",
    "    sigma = np.sqrt((sensitivity**2 * alpha) / (2 * epsilon))\n",
    "    return v + tf.random.normal(v.shape, mean=0.0, stddev=sigma)\n",
    "\n",
    "def tf_gaussian_mech_zCDP(v, sensitivity, rho):\n",
    "    sigma = np.sqrt((sensitivity**2) / (2 * rho))\n",
    "    return v + tf.random.normal(v.shape, mean=0.0, stddev=sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DPOptimizer(tf.keras.optimizers.Optimizer):\n",
    "    def __init__(self, epochs, b=3.0, learning_rate=0.01, name=\"DPOptimizer\", **kwargs):\n",
    "        super().__init__(name, **kwargs)\n",
    "        self._set_hyper(\"learning_rate\", learning_rate)\n",
    "        self.epochs = epochs\n",
    "        self.b = b\n",
    "    \n",
    "    def _create_slots(self, var_list):\n",
    "        pass\n",
    "\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {\n",
    "            **base_config,\n",
    "            \"learning_rate\": self._serialize_hyperparameter(\"learning_rate\"),\n",
    "        }\n",
    "\n",
    "    \n",
    "class EpsilonDeltaDPGradientDescent(DPOptimizer):\n",
    "    def __init__(self, epochs, epsilon, delta, b=3.0, learning_rate=0.01, name=\"EpsilonDeltaDPGradientDescent\", **kwargs):\n",
    "        DPOptimizer.__init__(self, epochs, b=b, learning_rate=learning_rate, name=name, **kwargs)        \n",
    "        self.epsilon = epsilon\n",
    "        self.delta = delta\n",
    "\n",
    "    @tf.function\n",
    "    def _resource_apply_dense(self, grad, var):\n",
    "        var_dtype = var.dtype.base_dtype\n",
    "        lr_t = self._decayed_lr(var_dtype)\n",
    "        \n",
    "        epsilon_i = self.epsilon / self.epochs\n",
    "        print(self.epochs)\n",
    "        delta_i = self.delta / self.epochs\n",
    "        print(delta_i)\n",
    "        \n",
    "        #clipped_grad = tf.math.reduce_mean(tf_l2_clip(grad, self.b), axis=0)\n",
    "        #clipped_grad = tf.numpy_function(lambda x: np.mean(x, axis=0), [tf_l2_clip(grad, self.b)], tf.float32)\n",
    "        #clipped_grad = tf.math.reduce_mean(tf.clip_by_norm(grad, self.b), axis=0)\n",
    "        clipped_grad = tf_l2_clip(grad, self.b)\n",
    "        new_var_m = var - tf_gaussian_mech(clipped_grad, self.b/len(x_train), epsilon_i, delta_i) * lr_t\n",
    "        #new_var_m = var - grad * lr_t\n",
    "        \n",
    "        new_var = new_var_m\n",
    "        var.assign(new_var)\n",
    "\n",
    "    \n",
    "class RenyiDPGradientDescent(DPOptimizer):\n",
    "    def __init__(self, epochs, alpha, epsilon_bar, b=3.0, learning_rate=0.01, name=\"RenyiDPGradientDescent\", **kwargs):\n",
    "        super().__init__(epochs,b=b, learning_rate=learning_rate, name=name, **kwargs)\n",
    "        self._set_hyper(\"learning_rate\", learning_rate)\n",
    "        \n",
    "        self.epsilon_bar = epsilon_bar\n",
    "        self.alpha = alpha\n",
    "        \n",
    "    @tf.function\n",
    "    def _resource_apply_dense(self, grad, var):\n",
    "        var_dtype = var.dtype.base_dtype\n",
    "        lr_t = self._decayed_lr(var_dtype)\n",
    "        \n",
    "        epsilon_bar_i = self.epsilon_bar / self.epochs\n",
    "         \n",
    "        clipped_grad = tf_l2_clip(grad, self.b)\n",
    "        new_var_m = var - tf_gaussian_mech_RDP(clipped_grad, self.b/len(x_train), self.alpha, epsilon_bar_i) * lr_t\n",
    "        \n",
    "        new_var = new_var_m\n",
    "        var.assign(new_var)\n",
    "\n",
    "        \n",
    "class ZeroConcentratedDPGradientDescent(DPOptimizer):\n",
    "    def __init__(self, epochs, rho, b=3.0, learning_rate=0.01, name=\"ZeroConcentratedDPGradientDescent\", **kwargs):\n",
    "        super().__init__(epochs,b=b, learning_rate=learning_rate, name=name, **kwargs)\n",
    "        self._set_hyper(\"learning_rate\", learning_rate)\n",
    "        \n",
    "        self.rho = rho\n",
    "        \n",
    "    @tf.function\n",
    "    def _resource_apply_dense(self, grad, var):\n",
    "        var_dtype = var.dtype.base_dtype\n",
    "        lr_t = self._decayed_lr(var_dtype)\n",
    "        \n",
    "        rho_i = self.rho / self.epochs\n",
    "         \n",
    "        clipped_grad = tf_l2_clip(grad, self.b)\n",
    "        new_var_m = var - tf_gaussian_mech_zCDP(clipped_grad, self.b/len(x_train), rho_i) * lr_t\n",
    "        \n",
    "        new_var = new_var_m\n",
    "        var.assign(new_var)\n",
    "        \n",
    "class PureDPGradientDescent(DPOptimizer):\n",
    "    def __init__(self, epochs, epsilon, b=3.0, learning_rate=0.01, name=\"PureDPGradientDescent\", **kwargs):\n",
    "        super().__init__(epochs,b=b, learning_rate=learning_rate, name=name, **kwargs)\n",
    "        self._set_hyper(\"learning_rate\", learning_rate)\n",
    "        \n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "    @tf.function\n",
    "    def _resource_apply_dense(self, grad, var):\n",
    "        var_dtype = var.dtype.base_dtype\n",
    "        lr_t = self._decayed_lr(var_dtype)\n",
    "        \n",
    "        epsilon_i = self.epsilon / self.epochs\n",
    "         \n",
    "        clipped_grad = tf_l2_clip(grad, self.b)\n",
    "        new_var_m = var - tf_laplace_mech(clipped_grad, self.b/len(x_train), epsilon_i) * lr_t\n",
    "        \n",
    "        new_var = new_var_m\n",
    "        var.assign(new_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
    "ed_dp = EpsilonDeltaDPGradientDescent(epochs=3, epsilon=0.1, delta=1e-5)\n",
    "r_dp = RenyiDPGradientDescent(epochs=1, alpha=500, epsilon_bar=0.001)\n",
    "zc_dp = ZeroConcentratedDPGradientDescent(epochs=5, rho=0.000001)\n",
    "pure_dp = PureDPGradientDescent(epochs=1, epsilon=.001)\n",
    "\n",
    "# we need to create new layers, otherwise scuffed\n",
    "model = Sequential([\n",
    "  Conv2D(28, kernel_size=(3,3), input_shape=input_shape),\n",
    "  MaxPooling2D(pool_size=(2, 2)),\n",
    "  Flatten(),\n",
    "  Dense(128, activation=tf.nn.relu),  \n",
    "  Dropout(0.3),\n",
    "  Dense(10,activation=tf.nn.softmax)\n",
    "])\n",
    "\n",
    "model.compile(optimizer=pure_dp, \n",
    "              loss='sparse_categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x=x_train,y=y_train, epochs=1, callbacks=[es], batch_size=64)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Effect of Noise on Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilons = np.linspace(.01, 100, 100)\n",
    "alphas = np.linspace(.00001, 5, 100)\n",
    "rhos = np.linspace(.00001, 5, 100)\n",
    "\n",
    "ep_de_opts = [EpsilonDeltaDPGradientDescent(epochs=3, epsilon=e, delta=1e-5) for e in epsilons]\n",
    "renyi_opts = [RenyiDPGradientDescent(epochs=1, alpha=a, epsilon_bar=0.001) for a in alphas ]\n",
    "zeroc_opts = [ZeroConcentratedDPGradientDescent(epochs=5, rho=r) for r in rhos]\n",
    "\n",
    "optimizer_data = {\n",
    "    \"EpsilonDelta\": {\n",
    "        \"batches\" : ep_de_opts,\n",
    "        \"accuracy\": []\n",
    "    },\n",
    "    \"Renyi\": {\n",
    "        \"batches\" : renyi_opts,\n",
    "        \"accuracy\": []\n",
    "    },\n",
    "    \"ZeroConc\": {\n",
    "        \"batches\" : zeroc_opts,\n",
    "        \"accuracy\": []\n",
    "    }\n",
    "}    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "3.3333333333333337e-06\n",
      "3\n",
      "3.3333333333333337e-06\n",
      "3\n",
      "3.3333333333333337e-06\n",
      "3\n",
      "3.3333333333333337e-06\n",
      "3\n",
      "3.3333333333333337e-06\n",
      "3\n",
      "3.3333333333333337e-06\n",
      "761/938 [=======================>......] - ETA: 10s - loss: 0.8028 - accuracy: 0.7735"
     ]
    }
   ],
   "source": [
    "for opt in optimizer_data:\n",
    "    for batch in optimizer_data[opt][\"batches\"]:\n",
    "\n",
    "        model = Sequential([\n",
    "          Conv2D(28, kernel_size=(3,3), input_shape=input_shape),\n",
    "          MaxPooling2D(pool_size=(2, 2)),\n",
    "          Flatten(),\n",
    "          Dense(128, activation=tf.nn.relu),  \n",
    "          Dropout(0.3),\n",
    "          Dense(10,activation=tf.nn.softmax)\n",
    "        ])\n",
    "\n",
    "        model.compile(optimizer=batch, \n",
    "                      loss='sparse_categorical_crossentropy', \n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "        model.fit(x=x_train,y=y_train, epochs=1, callbacks=[es], batch_size=64)\n",
    "        accuracy = model.evaluate(x_test, y_test)\n",
    "        optimizer_data[opt][\"accuracy\"].append(accuracy[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4,), dtype=float32, numpy=array([0.9289808, 1.0607969, 0.9809418, 0.9453022], dtype=float32)>"
      ]
     },
     "execution_count": 481,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = tf.constant([1.0, 1.0, 1.0, 1.0])\n",
    "tf_RDP_gaussian_mech(t, 0.0001, 500, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4,), dtype=float32, numpy=array([0.99968153, 1.000329  , 1.0001874 , 1.0001769 ], dtype=float32)>"
      ]
     },
     "execution_count": 482,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = tf.constant([1.0, 1.0, 1.0, 1.0])\n",
    "tf_gaussian_mech_zCDP(t, 0.0001, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
