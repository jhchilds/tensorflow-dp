{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Privacy Final Project\n",
    "### Sam Clark & Josh Childs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project, we've decided to compare the accuracy of several normal Convolutional Neural Networks to their counter parts that will use differential privacy. We will be using the MNIST dataset with the tensflow library.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v2 as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from ipywidgets import IntProgress\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "import pickle\n",
    "tf.enable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.compat.v1.distributions import Laplace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load MNIST Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
    "input_shape = (28, 28, 1)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 26, 26, 28)        280       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 13, 13, 28)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 4732)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               605824    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 607,394\n",
      "Trainable params: 607,394\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "  Conv2D(28, kernel_size=(3,3), input_shape=input_shape),\n",
    "  MaxPooling2D(pool_size=(2, 2)),\n",
    "  Flatten(),\n",
    "  Dense(128, activation=tf.nn.relu),  \n",
    "  Dropout(0.3),\n",
    "  Dense(10,activation=tf.nn.softmax)\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train & Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
    "\n",
    "# model.compile(optimizer='sgd',\n",
    "#               loss='sparse_categorical_crossentropy', \n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# model.fit(x=x_train,y=y_train, epochs=1, callbacks = [callback])\n",
    "# model.save(\"models/original\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Differential Privacy Optimizer Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_l2_clip(v, b):\n",
    "    norm = tf.norm(v, ord=2)\n",
    "    return tf.cond(norm > b, lambda: b * (v / norm), lambda: v)\n",
    "\n",
    "def laplace_mech(v, sensitivity, epsilon):\n",
    "    return v + np.random.laplace(loc=0, scale=sensitivity / epsilon)\n",
    "\n",
    "def tf_laplace_mech(v, sensitivity, epsilon):\n",
    "    return tf.numpy_function(laplace_mech, [v, sensitivity, epsilon], tf.float32)\n",
    "\n",
    "def tf_gaussian_mech(v, sensitivity, epsilon, delta):\n",
    "    return v + tf.random.normal(v.shape, mean=0.0, stddev=sensitivity * np.sqrt(2*np.log(1.25/delta)) / epsilon)\n",
    "\n",
    "def tf_gaussian_mech_RDP(v, sensitivity, alpha, epsilon):\n",
    "    sigma = np.sqrt((sensitivity**2 * alpha) / (2 * epsilon))\n",
    "    return v + tf.random.normal(v.shape, mean=0.0, stddev=sigma)\n",
    "\n",
    "def tf_gaussian_mech_zCDP(v, sensitivity, rho):\n",
    "    sigma = np.sqrt((sensitivity**2) / (2 * rho))\n",
    "    return v + tf.random.normal(v.shape, mean=0.0, stddev=sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DPOptimizer(tf.keras.optimizers.Optimizer):\n",
    "    def __init__(self, epochs, b=3.0, learning_rate=0.01, name=\"DPOptimizer\", **kwargs):\n",
    "        super().__init__(name, **kwargs)\n",
    "        self._set_hyper(\"learning_rate\", learning_rate)\n",
    "        self.epochs = epochs\n",
    "        self.b = b\n",
    "    \n",
    "    def _create_slots(self, var_list):\n",
    "        pass\n",
    "\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {\n",
    "            **base_config,\n",
    "            \"learning_rate\": self._serialize_hyperparameter(\"learning_rate\"),\n",
    "        }\n",
    "\n",
    "\n",
    "class NoDPGradientDescent(DPOptimizer):\n",
    "    def __init__(self, epochs, learning_rate=0.01, name=\"NoDPGradientDescent\", **kwargs):\n",
    "        DPOptimizer.__init__(self, epochs, learning_rate=learning_rate, name=name, **kwargs)        \n",
    "\n",
    "    @tf.function\n",
    "    def _resource_apply_dense(self, grad, var):\n",
    "        var_dtype = var.dtype.base_dtype\n",
    "        lr_t = self._decayed_lr(var_dtype)\n",
    "        \n",
    "        new_var_m = var - grad * lr_t\n",
    "        #new_var_m = var - grad * lr_t\n",
    "        \n",
    "        new_var = new_var_m\n",
    "        var.assign(new_var)\n",
    "    \n",
    "    \n",
    "class EpsilonDeltaDPGradientDescent(DPOptimizer):\n",
    "    def __init__(self, epochs, epsilon, delta, b=3.0, learning_rate=0.01, name=\"EpsilonDeltaDPGradientDescent\", **kwargs):\n",
    "        DPOptimizer.__init__(self, epochs, b=b, learning_rate=learning_rate, name=name, **kwargs)        \n",
    "        self.epsilon = epsilon\n",
    "        self.delta = delta\n",
    "\n",
    "    @tf.function\n",
    "    def _resource_apply_dense(self, grad, var):\n",
    "        var_dtype = var.dtype.base_dtype\n",
    "        lr_t = self._decayed_lr(var_dtype)\n",
    "        \n",
    "        epsilon_i = self.epsilon / self.epochs\n",
    "        delta_i = self.delta / self.epochs\n",
    "        \n",
    "        #clipped_grad = tf.math.reduce_mean(tf_l2_clip(grad, self.b), axis=0, keepdims=True)\n",
    "        #clipped_grad = tf.numpy_function(lambda x: np.mean(x, axis=0), [tf_l2_clip(grad, self.b)], tf.float32)\n",
    "        #clipped_grad = tf.math.reduce_mean(tf.clip_by_norm(grad, self.b), axis=0)\n",
    "        clipped_grad = tf_l2_clip(grad, self.b)\n",
    "        new_var_m = var - tf_gaussian_mech(clipped_grad, self.b/len(x_train), epsilon_i, delta_i) * lr_t\n",
    "        #new_var_m = var - grad * lr_t\n",
    "        \n",
    "        new_var = new_var_m\n",
    "        var.assign(new_var)\n",
    "\n",
    "    \n",
    "class RenyiDPGradientDescent(DPOptimizer):\n",
    "    def __init__(self, epochs, alpha, epsilon_bar, b=3.0, learning_rate=0.01, name=\"RenyiDPGradientDescent\", **kwargs):\n",
    "        super().__init__(epochs,b=b, learning_rate=learning_rate, name=name, **kwargs)\n",
    "        self._set_hyper(\"learning_rate\", learning_rate)\n",
    "        \n",
    "        self.epsilon_bar = epsilon_bar\n",
    "        self.alpha = alpha\n",
    "        \n",
    "    @tf.function\n",
    "    def _resource_apply_dense(self, grad, var):\n",
    "        var_dtype = var.dtype.base_dtype\n",
    "        lr_t = self._decayed_lr(var_dtype)\n",
    "        \n",
    "        epsilon_bar_i = self.epsilon_bar / self.epochs\n",
    "         \n",
    "        clipped_grad = tf_l2_clip(grad, self.b)\n",
    "        new_var_m = var - tf_gaussian_mech_RDP(clipped_grad, self.b/len(x_train), self.alpha, epsilon_bar_i) * lr_t\n",
    "        \n",
    "        new_var = new_var_m\n",
    "        var.assign(new_var)\n",
    "\n",
    "        \n",
    "class ZeroConcentratedDPGradientDescent(DPOptimizer):\n",
    "    def __init__(self, epochs, rho, b=3.0, learning_rate=0.01, name=\"ZeroConcentratedDPGradientDescent\", **kwargs):\n",
    "        super().__init__(epochs,b=b, learning_rate=learning_rate, name=name, **kwargs)\n",
    "        self._set_hyper(\"learning_rate\", learning_rate)\n",
    "        \n",
    "        self.rho = rho\n",
    "        \n",
    "    @tf.function\n",
    "    def _resource_apply_dense(self, grad, var):\n",
    "        var_dtype = var.dtype.base_dtype\n",
    "        lr_t = self._decayed_lr(var_dtype)\n",
    "        \n",
    "        rho_i = self.rho / self.epochs\n",
    "         \n",
    "        clipped_grad = tf_l2_clip(grad, self.b)\n",
    "        new_var_m = var - tf_gaussian_mech_zCDP(clipped_grad, self.b/len(x_train), rho_i) * lr_t\n",
    "        \n",
    "        new_var = new_var_m\n",
    "        var.assign(new_var)\n",
    "        \n",
    "class PureDPGradientDescent(DPOptimizer):\n",
    "    def __init__(self, epochs, epsilon, b=3.0, learning_rate=0.01, name=\"PureDPGradientDescent\", **kwargs):\n",
    "        super().__init__(epochs,b=b, learning_rate=learning_rate, name=name, **kwargs)\n",
    "        self._set_hyper(\"learning_rate\", learning_rate)\n",
    "        \n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "    @tf.function\n",
    "    def _resource_apply_dense(self, grad, var):\n",
    "        var_dtype = var.dtype.base_dtype\n",
    "        lr_t = self._decayed_lr(var_dtype)\n",
    "        \n",
    "        epsilon_i = self.epsilon / self.epochs\n",
    "         \n",
    "        clipped_grad = tf_l2_clip(grad, self.b)\n",
    "        new_var_m = var - tf_laplace_mech(clipped_grad, self.b/len(x_train), epsilon_i) * lr_t\n",
    "        \n",
    "        new_var = new_var_m\n",
    "        var.assign(new_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# es = callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
    "# ed_dp = EpsilonDeltaDPGradientDescent(epochs=3, epsilon=0.1, delta=1e-5)\n",
    "# r_dp = RenyiDPGradientDescent(epochs=1, alpha=500, epsilon_bar=0.001)\n",
    "# zc_dp = ZeroConcentratedDPGradientDescent(epochs=5, rho=0.000001)\n",
    "# pure_dp = PureDPGradientDescent(epochs=1, epsilon=.001)\n",
    "\n",
    "# # we need to create new layers, otherwise scuffed\n",
    "# model = Sequential([\n",
    "#   Conv2D(28, kernel_size=(3,3), input_shape=input_shape),\n",
    "#   MaxPooling2D(pool_size=(2, 2)),\n",
    "#   Flatten(),\n",
    "#   Dense(128, activation=tf.nn.relu),  \n",
    "#   Dropout(0.3),\n",
    "#   Dense(10,activation=tf.nn.softmax)\n",
    "# ])\n",
    "\n",
    "# model.compile(optimizer=pure_dp, \n",
    "#               loss='sparse_categorical_crossentropy', \n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# model.fit(x=x_train,y=y_train, epochs=1, callbacks=[es], batch_size=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.7262 - accuracy: 0.7966\n",
      "Epoch 2/10\n",
      "938/938 [==============================] - 1s 974us/step - loss: 0.3180 - accuracy: 0.9060\n",
      "Epoch 3/10\n",
      "938/938 [==============================] - 1s 968us/step - loss: 0.2537 - accuracy: 0.9254\n",
      "Epoch 4/10\n",
      "938/938 [==============================] - 1s 967us/step - loss: 0.2195 - accuracy: 0.9353\n",
      "Epoch 5/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.1952 - accuracy: 0.9435\n",
      "Epoch 6/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.1775 - accuracy: 0.9478\n",
      "Epoch 7/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.1662 - accuracy: 0.9495\n",
      "Epoch 8/10\n",
      "938/938 [==============================] - 1s 990us/step - loss: 0.1553 - accuracy: 0.9538\n",
      "Epoch 9/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.1470 - accuracy: 0.9560\n",
      "Epoch 10/10\n",
      "938/938 [==============================] - 1s 986us/step - loss: 0.1408 - accuracy: 0.9586\n",
      "313/313 [==============================] - 0s 647us/step - loss: 0.1002 - accuracy: 0.9695\n",
      "[0.10020816326141357, 0.9695000052452087]\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "  Conv2D(28, kernel_size=(3,3), input_shape=input_shape),\n",
    "  MaxPooling2D(pool_size=(2, 2)),\n",
    "  Flatten(),\n",
    "  Dense(128, activation=tf.nn.relu),  \n",
    "  Dropout(0.3),\n",
    "  Dense(10,activation=tf.nn.softmax)\n",
    "])\n",
    "\n",
    "model.compile(optimizer=NoDPGradientDescent(epochs=10), \n",
    "              loss='sparse_categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x=x_train,y=y_train, epochs=10, batch_size=64)\n",
    "accuracy = model.evaluate(x_test, y_test)\n",
    "print(accuracy)\n",
    "\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Effect of Noise on Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "rhos = np.linspace(0.0000001, 0.00001, 100)\n",
    "\n",
    "epsilons = []\n",
    "for rho in rhos:\n",
    "    epsilon = rho + 2 * np.sqrt(rho * np.log(1 / 1e-5))\n",
    "    epsilons.append(epsilon)\n",
    "\n",
    "epsilon_bars = []\n",
    "for epsilon in epsilons:\n",
    "    epsilon_bar = epsilon - np.log(1 / 1e-5) / (10000 - 1)\n",
    "    epsilon_bars.append(epsilon_bar)\n",
    "    \n",
    "# print(rhos)\n",
    "# print(epsilons)\n",
    "# print(epsilon_bars)\n",
    "\n",
    "ep_de_opts = [EpsilonDeltaDPGradientDescent(epochs=10, epsilon=e, delta=1e-5) for e in epsilons]\n",
    "renyi_opts = [RenyiDPGradientDescent(epochs=10, alpha=100, epsilon_bar=e_b) for e_b in epsilon_bars]\n",
    "zeroc_opts = [ZeroConcentratedDPGradientDescent(epochs=10, rho=r) for r in rhos]\n",
    "pured_opts = [PureDPGradientDescent(epochs=10, epsilon=e) for e in epsilons]\n",
    "\n",
    "optimizer_data = {\n",
    "    \"EpsilonDelta\": {\n",
    "        \"batches\" : ep_de_opts,\n",
    "        \"accuracy\": []\n",
    "    },\n",
    "    \"Renyi\": {\n",
    "        \"batches\" : renyi_opts,\n",
    "        \"accuracy\": []\n",
    "    },\n",
    "    \"ZeroConc\": {\n",
    "        \"batches\" : zeroc_opts,\n",
    "        \"accuracy\": []\n",
    "    },\n",
    "    \"Pure\": {\n",
    "        \"batches\": pured_opts,\n",
    "        \"accuracy\": []\n",
    "    }\n",
    "}    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "EpsilonDelta\n",
      "\n",
      "BATCH 0/100\n",
      "--------\n",
      "Epoch 1/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 2.1687 - accuracy: 0.4000\n",
      "Epoch 2/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 2.2030 - accuracy: 0.3056\n",
      "Epoch 3/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 2.3777 - accuracy: 0.1704\n",
      "Epoch 4/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 2.3192 - accuracy: 0.1463\n",
      "Epoch 5/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 2.3243 - accuracy: 0.1597\n",
      "Epoch 6/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 2.4023 - accuracy: 0.1180\n",
      "Epoch 7/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 2.3657 - accuracy: 0.1397\n",
      "Epoch 8/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 2.3557 - accuracy: 0.1398\n",
      "Epoch 9/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 2.3815 - accuracy: 0.1383\n",
      "Epoch 10/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 2.4328 - accuracy: 0.1181\n",
      "313/313 [==============================] - 0s 642us/step - loss: 2.5125 - accuracy: 0.1281\n",
      "BATCH ACCURACY: [2.512462615966797, 0.12809999287128448]\n",
      "BATCH 1/100\n",
      "--------\n",
      "Epoch 1/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 1.5003 - accuracy: 0.5476\n",
      "Epoch 2/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 1.4459 - accuracy: 0.5825\n",
      "Epoch 3/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 1.5469 - accuracy: 0.5230\n",
      "Epoch 4/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 1.6879 - accuracy: 0.4594\n",
      "Epoch 5/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 1.7315 - accuracy: 0.4420\n",
      "Epoch 6/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 1.7353 - accuracy: 0.4090\n",
      "Epoch 7/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 1.8058 - accuracy: 0.3863\n",
      "Epoch 8/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 1.8269 - accuracy: 0.3873\n",
      "Epoch 9/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 1.8879 - accuracy: 0.3397\n",
      "Epoch 10/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 1.8986 - accuracy: 0.3376\n",
      "313/313 [==============================] - 0s 659us/step - loss: 1.5839 - accuracy: 0.4826\n",
      "BATCH ACCURACY: [1.5839492082595825, 0.48260000348091125]\n",
      "BATCH 2/100\n",
      "--------\n",
      "Epoch 1/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 1.2634 - accuracy: 0.6012\n",
      "Epoch 2/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 1.1332 - accuracy: 0.6667\n",
      "Epoch 3/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 1.2038 - accuracy: 0.6414\n",
      "Epoch 4/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 1.2082 - accuracy: 0.6295\n",
      "Epoch 5/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 1.2856 - accuracy: 0.6021\n",
      "Epoch 6/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 1.3076 - accuracy: 0.6012\n",
      "Epoch 7/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 1.3403 - accuracy: 0.5847\n",
      "Epoch 8/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 1.3691 - accuracy: 0.5678\n",
      "Epoch 9/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 1.4333 - accuracy: 0.5486\n",
      "Epoch 10/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 1.4496 - accuracy: 0.5400\n",
      "313/313 [==============================] - 0s 647us/step - loss: 1.0602 - accuracy: 0.6543\n",
      "BATCH ACCURACY: [1.060197353363037, 0.6542999744415283]\n",
      "BATCH 3/100\n",
      "--------\n",
      "Epoch 1/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 1.1898 - accuracy: 0.6083\n",
      "Epoch 2/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.9951 - accuracy: 0.6911\n",
      "Epoch 3/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 1.0211 - accuracy: 0.6897\n",
      "Epoch 4/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 1.0261 - accuracy: 0.6844\n",
      "Epoch 5/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 1.0493 - accuracy: 0.6734\n",
      "Epoch 6/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 1.1210 - accuracy: 0.6560\n",
      "Epoch 7/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 1.2050 - accuracy: 0.6285\n",
      "Epoch 8/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 1.2182 - accuracy: 0.6243\n",
      "Epoch 9/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 1.2310 - accuracy: 0.6143\n",
      "Epoch 10/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 1.2473 - accuracy: 0.6095\n",
      "313/313 [==============================] - 0s 648us/step - loss: 0.9014 - accuracy: 0.7156\n",
      "BATCH ACCURACY: [0.90144944190979, 0.7156000137329102]\n",
      "BATCH 4/100\n",
      "--------\n",
      "Epoch 1/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 1.1162 - accuracy: 0.6341\n",
      "Epoch 2/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.8774 - accuracy: 0.7406\n",
      "Epoch 3/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.8952 - accuracy: 0.7342\n",
      "Epoch 4/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.8932 - accuracy: 0.7366\n",
      "Epoch 5/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.9298 - accuracy: 0.7284\n",
      "Epoch 6/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.9492 - accuracy: 0.7170\n",
      "Epoch 7/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.9861 - accuracy: 0.7056\n",
      "Epoch 8/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 1.0283 - accuracy: 0.6904\n",
      "Epoch 9/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 1.0259 - accuracy: 0.6903\n",
      "Epoch 10/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 1.0327 - accuracy: 0.6869\n",
      "313/313 [==============================] - 0s 653us/step - loss: 0.6680 - accuracy: 0.8082\n",
      "BATCH ACCURACY: [0.6680459380149841, 0.8082000017166138]\n",
      "BATCH 5/100\n",
      "--------\n",
      "Epoch 1/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 1.0433 - accuracy: 0.6611\n",
      "Epoch 2/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.8349 - accuracy: 0.7465\n",
      "Epoch 3/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.8205 - accuracy: 0.7530\n",
      "Epoch 4/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.8187 - accuracy: 0.7560\n",
      "Epoch 5/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.8272 - accuracy: 0.7556\n",
      "Epoch 6/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.8741 - accuracy: 0.7358\n",
      "Epoch 7/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.9108 - accuracy: 0.7246\n",
      "Epoch 8/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.9009 - accuracy: 0.7304\n",
      "Epoch 9/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.9190 - accuracy: 0.7206\n",
      "Epoch 10/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.9326 - accuracy: 0.7169\n",
      "313/313 [==============================] - 0s 675us/step - loss: 0.6668 - accuracy: 0.7993\n",
      "BATCH ACCURACY: [0.666827917098999, 0.7993000149726868]\n",
      "BATCH 6/100\n",
      "--------\n",
      "Epoch 1/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.9868 - accuracy: 0.6820\n",
      "Epoch 2/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.7331 - accuracy: 0.7743\n",
      "Epoch 3/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.7595 - accuracy: 0.7731\n",
      "Epoch 4/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.7705 - accuracy: 0.7689\n",
      "Epoch 5/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.8035 - accuracy: 0.7559\n",
      "Epoch 6/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.8102 - accuracy: 0.7519\n",
      "Epoch 7/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.8274 - accuracy: 0.7514\n",
      "Epoch 8/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.8516 - accuracy: 0.7404\n",
      "Epoch 9/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.8667 - accuracy: 0.7364\n",
      "Epoch 10/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.8442 - accuracy: 0.7467\n",
      "313/313 [==============================] - 0s 632us/step - loss: 0.5472 - accuracy: 0.8371\n",
      "BATCH ACCURACY: [0.5471539497375488, 0.8371000289916992]\n",
      "BATCH 7/100\n",
      "--------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.9895 - accuracy: 0.6829\n",
      "Epoch 2/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.7110 - accuracy: 0.7821\n",
      "Epoch 3/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.7240 - accuracy: 0.7831\n",
      "Epoch 4/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.7252 - accuracy: 0.7812\n",
      "Epoch 5/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.7383 - accuracy: 0.7780\n",
      "Epoch 6/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.7376 - accuracy: 0.7766\n",
      "Epoch 7/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.7799 - accuracy: 0.7662\n",
      "Epoch 8/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.8057 - accuracy: 0.7582\n",
      "Epoch 9/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.8219 - accuracy: 0.7514\n",
      "Epoch 10/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.8217 - accuracy: 0.7554\n",
      "313/313 [==============================] - 0s 694us/step - loss: 0.5459 - accuracy: 0.8339\n",
      "BATCH ACCURACY: [0.545879065990448, 0.833899974822998]\n",
      "BATCH 8/100\n",
      "--------\n",
      "Epoch 1/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 1.0061 - accuracy: 0.6714\n",
      "Epoch 2/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.6470 - accuracy: 0.8001\n",
      "Epoch 3/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.6388 - accuracy: 0.8065\n",
      "Epoch 4/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.6412 - accuracy: 0.8038\n",
      "Epoch 5/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.6761 - accuracy: 0.7955\n",
      "Epoch 6/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.6831 - accuracy: 0.7931\n",
      "Epoch 7/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.6967 - accuracy: 0.7902\n",
      "Epoch 8/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.6931 - accuracy: 0.7914\n",
      "Epoch 9/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.7017 - accuracy: 0.7857\n",
      "Epoch 10/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.7255 - accuracy: 0.7818\n",
      "313/313 [==============================] - 0s 693us/step - loss: 0.4214 - accuracy: 0.8770\n",
      "BATCH ACCURACY: [0.42139098048210144, 0.8769999742507935]\n",
      "BATCH 9/100\n",
      "--------\n",
      "Epoch 1/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.9495 - accuracy: 0.6970\n",
      "Epoch 2/10\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.6308 - accuracy: 0.8016\n",
      "Epoch 3/10\n",
      "302/938 [========>.....................] - ETA: 0s - loss: 0.6259 - accuracy: 0.8058"
     ]
    }
   ],
   "source": [
    "for opt in optimizer_data:\n",
    "    print(\"----------------------------------------\")\n",
    "    print(opt)\n",
    "    print()\n",
    "    \n",
    "    for i, batch in enumerate(optimizer_data[opt][\"batches\"]):\n",
    "        \n",
    "        print(f\"BATCH {i}/{len(optimizer_data[opt]['batches'])}\")\n",
    "        print(\"--------\")\n",
    "\n",
    "        model = Sequential([\n",
    "          Conv2D(28, kernel_size=(3,3), input_shape=input_shape),\n",
    "          MaxPooling2D(pool_size=(2, 2)),\n",
    "          Flatten(),\n",
    "          Dense(128, activation=tf.nn.relu),  \n",
    "          Dropout(0.3),\n",
    "          Dense(10,activation=tf.nn.softmax)\n",
    "        ])\n",
    "\n",
    "        model.compile(optimizer=batch, \n",
    "                      loss='sparse_categorical_crossentropy', \n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "        model.fit(x=x_train,y=y_train, epochs=10, batch_size=64)\n",
    "        accuracy = model.evaluate(x_test, y_test)\n",
    "        optimizer_data[opt][\"accuracy\"].append(accuracy[1])\n",
    "              \n",
    "        print(f\"BATCH ACCURACY: {accuracy}\")\n",
    "        \n",
    "        tf.keras.backend.clear_session()\n",
    "    \n",
    "    pickle.dump(optimizer_data, open(f\"{opt}_results.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(100):\n",
    "    print(\"DONE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = tf.constant([1.0, 1.0, 1.0, 1.0])\n",
    "tf_RDP_gaussian_mech(t, 0.0001, 500, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4,), dtype=float32, numpy=array([1.0002027 , 0.99996525, 1.0000987 , 1.0000061 ], dtype=float32)>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = tf.constant([1.0, 1.0, 1.0, 1.0])\n",
    "tf_gaussian_mech_zCDP(t, 0.0001, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4,), dtype=float32, numpy=array([ 115.59105 ,  316.6945  , -122.92863 ,  -63.800293], dtype=float32)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = tf.constant([1.0, 1.0, 1.0, 1.0])\n",
    "tf_gaussian_mech_RDP(t, 1.0, 5, 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=1.0>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.math.reduce_mean(t, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "938/938 [==============================] - 1s 885us/step - loss: 2.1672 - accuracy: 0.2213\n",
      "Epoch 2/10\n",
      "938/938 [==============================] - 1s 886us/step - loss: 2.0692 - accuracy: 0.3365\n",
      "Epoch 3/10\n",
      "938/938 [==============================] - 1s 907us/step - loss: 2.0478 - accuracy: 0.3722\n",
      "Epoch 4/10\n",
      "938/938 [==============================] - 1s 888us/step - loss: 2.0401 - accuracy: 0.38310s - loss: 2.0412 - \n",
      "Epoch 5/10\n",
      "938/938 [==============================] - 1s 879us/step - loss: 2.0350 - accuracy: 0.3899\n",
      "Epoch 6/10\n",
      "938/938 [==============================] - 1s 867us/step - loss: 2.0305 - accuracy: 0.3949\n",
      "Epoch 7/10\n",
      "938/938 [==============================] - 1s 942us/step - loss: 2.0261 - accuracy: 0.4008\n",
      "Epoch 8/10\n",
      "938/938 [==============================] - 1s 882us/step - loss: 2.0215 - accuracy: 0.4049\n",
      "Epoch 9/10\n",
      "938/938 [==============================] - 1s 899us/step - loss: 2.0168 - accuracy: 0.4092\n",
      "Epoch 10/10\n",
      "938/938 [==============================] - 1s 920us/step - loss: 2.0120 - accuracy: 0.4118\n",
      "313/313 [==============================] - 0s 644us/step - loss: 2.0088 - accuracy: 0.4125\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "  Flatten(input_shape=(28, 28, 1)),\n",
    "  Dense(128, activation='relu'),\n",
    "  Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer=EpsilonDeltaDPGradientDescent(epochs=10, epsilon=10000000.0, delta=1e-5),\n",
    "              loss='sparse_categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x=x_train,y=y_train, epochs=10, batch_size=64)\n",
    "accuracy = model.evaluate(x_test, y_test)\n",
    "optimizer_data[opt][\"accuracy\"].append(accuracy[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
